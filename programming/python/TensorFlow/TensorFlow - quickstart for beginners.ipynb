{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-chinese",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial: Quickstart for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "determined-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:36:59.521175: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-01 19:36:59.568571: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-01 19:36:59.569370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 19:37:00.484336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-crossing",
   "metadata": {},
   "source": [
    "Load & prepare MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalize data\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-policy",
   "metadata": {},
   "source": [
    "Build a sequential neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    tf.keras.layers.Dense(128, activation='relu'), \n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-bubble",
   "metadata": {},
   "source": [
    "For each example, the model returns logits, or log-odds scores for each class\n",
    "\n",
    "**logits**: non-normalized predictions (usually input to softmax which normalizes the probabilities)<br> \n",
    "\n",
    "**log-odds**: logarithm of the odds of an event: $\\text{log-odds} = \\ln(\\frac{p}{1-p})$. This is the inverse of sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "promising-controversy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12894067, -0.3003755 ,  0.1621922 , -0.84611845, -0.05044453,\n",
       "         0.24006858, -0.0144949 , -0.17953241,  0.00139112,  0.17785925]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-approach",
   "metadata": {},
   "source": [
    "Now use `tf.nn.softmax` to convert to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "balanced-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09287914, 0.07824651, 0.12426694, 0.0453369 , 0.10046358,\n",
       "        0.1343312 , 0.10414091, 0.08829711, 0.1058085 , 0.12622917]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-arcade",
   "metadata": {},
   "source": [
    "**Note**: you can put tf.nn.softmax as the last layer. Discouraged b/c softmax output makes it impossible to provide an exact / stable loss calculation. \n",
    "\n",
    "`losses.SparseCategoricalCrossentropy` - take a vector of logits & returns a scalar loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "novel-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-blend",
   "metadata": {},
   "source": [
    "This is the negative log probability of the true class: zero if model is 100% sure that this is the correct class. \n",
    "\n",
    "Since the model is untrained, the probabilities will be close to random, so loss should be log(1/10) ~ 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "asian-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(), dtype=float32, numpy=2.0074468>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-thirty",
   "metadata": {},
   "source": [
    "Now compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "objective-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "             loss=loss_fn, \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smart-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3008 - accuracy: 0.9133\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1431 - accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1058 - accuracy: 0.9680\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0849 - accuracy: 0.9734\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0727 - accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f60ecb92260>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-rogers",
   "metadata": {},
   "source": [
    "`Model.evaluate` - check performance on a validation or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "swedish-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0705 - accuracy: 0.9789 - 598ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07050502300262451, 0.9789000153541565]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
