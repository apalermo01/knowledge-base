**Tokenization**: This split each text into words / tokens. The actual details vary based on model - each token may be a common word, syllable, or individual letters.

**lemmatization**: Converting words to its "base" form (ex: converting "caring" to "care")

**stemming**: a crude (and faster) form of lemmatization that chops prefixes / suffixes to try to get a word close to its "base" form

**stop words removal**: stopwords are common words like "we", "are", "the", etc. Sometimes 


# References
https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners
https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79
https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html