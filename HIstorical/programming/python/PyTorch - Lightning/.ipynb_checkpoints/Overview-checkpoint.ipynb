{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f3f248",
   "metadata": {},
   "source": [
    "# PyTorch lightning training intro\n",
    "\n",
    "https://www.pytorchlightning.ai/tutorials\n",
    "\n",
    "- LightningModule - full DL training system (e.g. model, collection of models (GAN), etc.)\n",
    "    - groups everything together into one self-contained system\n",
    "    - collection of methods\n",
    "- see examples in repo for use-cases\n",
    "- autoencoder: encoder and decoder (2 networks)\n",
    "\n",
    "**defining the module:**<br> \n",
    "- initialize model(s) in init\n",
    "- in forward method, define how it will work in inference\n",
    "- train_step: define what happens in train step\n",
    "- validation_step, test_step: do same thing\n",
    "- configure_optimizers: defines optimizer\n",
    "\n",
    "No looping over epochs or datasets, trainer does that for us\n",
    "\n",
    "**Defining dataset**<br> \n",
    "- works directly with dataloaders\n",
    "- can always modify a hook / callback to modify functionality to make it do what you need. \n",
    "\n",
    "need 3 things: LightningModule, DataSet, Trainer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd01bc",
   "metadata": {},
   "source": [
    "# Automatic Batch Size Finder\n",
    "\n",
    "- `auto_scale_batch_size` - finds largest batch size that can fit on GPU\n",
    "- pass this flag when initializing trainer\n",
    "- must have `self.batch_size` or `self.save_hyperparams()` in module\n",
    "- can hard code this into DataLoader after first pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911aeca3",
   "metadata": {},
   "source": [
    "# Automatic Learning Rate Finder\n",
    "\n",
    "- most important hyperparameter\n",
    "- `auto_lr_find` - outputs lr vs loss\n",
    "    - note: we want the lr where this graph is the steepest\n",
    "- configure_optimizer won't be called until needed for training (lazy initialization)\n",
    "- run `trainer.tune(model)` to find best learning rate\n",
    "- then run training with that initial lr\n",
    "- only works with single optimizer for now\n",
    "- \"learning rate may be impossible to find with some problems\" - can use as ballpark estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f51d9",
   "metadata": {},
   "source": [
    "# Exploding and Vanishing Gradients\n",
    "\n",
    "- `track_grad_norm` - set to l(n) norm to track norm of the lr throughout training to look for exploding / vanishing gradients. \n",
    "- `gradient_clip_val` - keeps exploding gradients from exploding to infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf98554",
   "metadata": {},
   "source": [
    "# Truncated Back-Propogation Through Time\n",
    "\n",
    "- `truncated_bptt_steps` - for long sequences, we can backprogate through a small subset of the of the network at a time\n",
    "- set flag to number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee02db6",
   "metadata": {},
   "source": [
    "# Reload DataLoaders Every Epoch\n",
    "\n",
    "- `reload_dataloaders_every_epoch` - loads dataset once by default. Setting this to True re-loads the data every time (use if data is changing or model is deployed in production)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1a6ee",
   "metadata": {},
   "source": [
    "# Lightning Callbacks\n",
    "\n",
    "- arbitrary self-contained programs.\n",
    "- gives full control over any section of training. \n",
    "- ex: early stopping, saving model checkpoints\n",
    "- can pass in callbacks when initializing trainer\n",
    "- specify which hook in which the callback gets called.\n",
    "- **hook** - a function that will be called at some point during training (over 40). \n",
    "- common callbacks found in PytorchLightning- bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4680dd",
   "metadata": {},
   "source": [
    "# Lightning Early Stopping\n",
    "\n",
    "- automatically stops training once some metric has stopped improving\n",
    "- default: happens when there has been no improvement in 3 evaluations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd71c8",
   "metadata": {},
   "source": [
    "# Lightning Weights Summary\n",
    "\n",
    "- `weights_summary` - prints a summary of weights\n",
    "- set to 'full' for full description of weights (similar to torchinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b0d29",
   "metadata": {},
   "source": [
    "# Lightning Progress Bar\n",
    "\n",
    "- `progress_bar_refresh_rate` - how often bar refreshes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b449fb5",
   "metadata": {},
   "source": [
    "# Lightning Profiler\n",
    "\n",
    "- identifies bottlenecks in training. gives high level description of method calls and how long those calls took. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7413c",
   "metadata": {},
   "source": [
    "# Controlling Lightning Training and Eval Loops\n",
    "\n",
    "- `min_epochs`, `max_epochs` flags\n",
    "- `min_steps`, `max_steps` (mini-batch steps)- validation steps do not count\n",
    "- `check_val_every_n_epochs` - how many train epochs to run before doing the validation step\n",
    "- `check_val_interval` - runs through validation loop within an epoch (good for cases where there is a very long training loop). \n",
    "- `num_sanity_val_steps` - will run this many rounds of validation before training starts (good for debugging). Runs 2 automatically, set to -1 to run entire val loop\n",
    "- `limit_train_batches`, `limit_val_batches`, `limit_train_batches` - shorten length of loops so we can debug val or test loop without having to wait for train loop to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812e8a5",
   "metadata": {},
   "source": [
    "# Training on GPUs\n",
    "\n",
    "- core: massive scaling w/ GPUs\n",
    "- PL can train on GPU or CPU with no changes in code\n",
    "- todo to get this hardware agnosticism: \n",
    "    - delete .cuda(), .to('cuda'), etc.\n",
    "    - when initializing tensors, always use `device = self.device`\n",
    "    - can also use type_as\n",
    "- use `gpus` flag to specify number of gpus and/or which specific GPUs to use\n",
    "    - -1 for all available GPUs\n",
    "- `log_gpu_memory` - logs memory usage per gpu (uses nvidia-smi under the hood). Turn it on when testing\n",
    "- `benchmark` - can result in speed-ups in input size is the same\n",
    "- `deterministic` - removes randomness from training (False by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f38987",
   "metadata": {},
   "source": [
    "# Training on multiple GPU nodes\n",
    "\n",
    "- single machine = node\n",
    "- `num_nodes` - specify number of nodes (computers) to use\n",
    "    - effective batch size = batch_size * num nodes * num gpus\n",
    "- many algos that can sync gradients\n",
    "- default: DistributedDataParallel (implemented by pytorch)\n",
    "    - each GPU gets a fraction of the dataset and a copy of the model\n",
    "    - perform forward pass in each GPU\n",
    "    - then sync the gradients between nodes\n",
    "    - then each optimizer in each node performs gradient descent\n",
    "    - IT IS VERY IMPORTANT TO INITIALIZE WITH A SEED (since each copy of the model is updated individually)\n",
    "    - use DDP whenever possible\n",
    "    - but it's not supported in windows\n",
    "    - can use DataParallel if no other option\n",
    "    - lightning also has ddp2 (see docs)\n",
    "- `ddp_cpu` - simulates ddp in a cpu for debugging. set num_processes to num gpus to simulate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc10df0",
   "metadata": {},
   "source": [
    "# Training on TPUs\n",
    "\n",
    "- Tensor Processing Units. Optimized for 128x128 matrix multiplication\n",
    "- 1 TPU = 4 V100s\n",
    "- use `tpu_cores` flag (no code changes needed)\n",
    "- still experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0aeb8",
   "metadata": {},
   "source": [
    "# Debugging Flags\n",
    "\n",
    "- `fast_dev_run` - touches every piece of code in model, train, val, and test loop for debugging. No logs, no checkpoints. \n",
    "- `overfit_batches` - take a single batch of train data and overfit it as much as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1c0f5",
   "metadata": {},
   "source": [
    "# Accumulating Gradients\n",
    "\n",
    "- `accumulate_grad_batches` - simulate a large batch by running several forward steps and accumulate the gradient so that it sees enough examples per batch. \n",
    "- can customize how much to accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954d4eb",
   "metadata": {},
   "source": [
    "# Mixed Precision Training\n",
    "\n",
    "- many models can train with a combo of 32 and 16 bit precision without loosing accuracy\n",
    "- reduces memory requirements\n",
    "- can get a speedup on some GPUs\n",
    "- PyTorch has automatic mixed precision support\n",
    "- `precision` - switch between 16 and 32 bit precision\n",
    "- `amp_level` - if using apex library, can set different modes for mixed precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
