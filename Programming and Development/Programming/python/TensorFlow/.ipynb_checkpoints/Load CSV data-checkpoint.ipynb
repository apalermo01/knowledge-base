{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f23358",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial on loading csv data\n",
    "\n",
    "2 main parts:\n",
    "\n",
    "- load data off the disk\n",
    "- preprocess into a form suitable for training\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745d6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 06:54:11.714050: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 06:54:18.445299: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 06:54:18.470765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 06:54:23.108109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205326b",
   "metadata": {},
   "source": [
    "Small datasets: load into memory as a pandas dataframe or numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6cf8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.3545</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.3740</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.435     0.335   0.110         0.334          0.1355          0.0775   \n",
       "1   0.585     0.450   0.125         0.874          0.3545          0.2075   \n",
       "2   0.655     0.510   0.160         1.092          0.3960          0.2825   \n",
       "3   0.545     0.425   0.125         0.768          0.2940          0.1495   \n",
       "4   0.545     0.420   0.130         0.879          0.3740          0.1695   \n",
       "\n",
       "   Shell weight  Age  \n",
       "0        0.0965    7  \n",
       "1        0.2250    6  \n",
       "2        0.3700   14  \n",
       "3        0.2600   16  \n",
       "4        0.2300   13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_train = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "\n",
    "abalone_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dcee36",
   "metadata": {},
   "source": [
    "task: predict age from other measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8c0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_features = abalone_train.copy()\n",
    "abalone_labels = abalone_features.pop('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b69b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.435, 0.335, 0.11 , ..., 0.136, 0.077, 0.097],\n",
       "       [0.585, 0.45 , 0.125, ..., 0.354, 0.207, 0.225],\n",
       "       [0.655, 0.51 , 0.16 , ..., 0.396, 0.282, 0.37 ],\n",
       "       ...,\n",
       "       [0.53 , 0.42 , 0.13 , ..., 0.374, 0.167, 0.249],\n",
       "       [0.395, 0.315, 0.105, ..., 0.118, 0.091, 0.119],\n",
       "       [0.45 , 0.355, 0.12 , ..., 0.115, 0.067, 0.16 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features = np.array(abalone_features)\n",
    "abalone_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebb534",
   "metadata": {},
   "source": [
    "Make a simple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae25f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3320, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d45e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_model = tf.keras.Sequential([\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1335d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 1s 1ms/step - loss: 64.5236\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 12.5431\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 8.6530\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 8.1424\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.6984\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.3298\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.0359\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.8151\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.6549\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.5361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1ffd36440>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078f177",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7554ebb",
   "metadata": {},
   "source": [
    "Keras has a normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9593389",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = layers.Normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0196c",
   "metadata": {},
   "source": [
    "Use `Normalization.adapt` to run normalization. I believe this is similar to scikit-learn's `fit` method. Put the normalize class in the sequential model to run transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2dbf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 1s 1ms/step - loss: 67.3003\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 13.1556\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 9.1493\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 8.5371\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.9927\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.5456\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 7.2060\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.9687\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.7833\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 6.6603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1fc125ff0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_abalone_model = tf.keras.Sequential([\n",
    "    normalize,\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                           optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b2a69",
   "metadata": {},
   "source": [
    "### Mixed datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eba13fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140e7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = titanic.copy()\n",
    "titanic_labels = titanic_features.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7d10a",
   "metadata": {},
   "source": [
    "Now we can't simply put it in a numpy array and pass it to a sequential model.\n",
    "\n",
    "**Benefit of Keras models: you bring the preprocessing steps with you when you save the model**\n",
    "\n",
    "Here, we'll build a model using the functional api\n",
    "\n",
    "This operates on \"symbolic\" tensors. \"eager\" tensors have a value, symbolic tensors do not. These symbolic tensors keep track of which operations run on them, then build a representation of the calculation\n",
    "\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95122175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None,) dtype=float32 (created by layer 'tf.__operators__.add')>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a symbolic input\n",
    "input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
    "\n",
    "# perform a calculation\n",
    "result = 2*input + 1\n",
    "\n",
    "# result doesn't have a value\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f07eb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = tf.keras.Model(inputs=input, outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb699add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d755bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329a9b9",
   "metadata": {},
   "source": [
    "We'll start by building a set of symbolic objects matching the names and datatypes of the CSV columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a00d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sex': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'sex')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'age')>,\n",
       " 'n_siblings_spouses': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'n_siblings_spouses')>,\n",
       " 'parch': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'parch')>,\n",
       " 'fare': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'fare')>,\n",
       " 'class': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'class')>,\n",
       " 'deck': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'deck')>,\n",
       " 'embark_town': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'embark_town')>,\n",
       " 'alone': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'alone')>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {}\n",
    "\n",
    "for name, column in titanic_features.items():\n",
    "    dtype = column.dtype\n",
    "    if dtype == object:\n",
    "        dtype = tf.string\n",
    "    else:\n",
    "        dtype = tf.float32\n",
    "    \n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "    \n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302dbf2",
   "metadata": {},
   "source": [
    "step 1: concatenate numeric inputs together and run through normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4adc076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'normalization_2')>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}\n",
    "\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
    "all_numeric_inputs = norm(x)\n",
    "all_numeric_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb5f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_inputs = [all_numeric_inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecf524",
   "metadata": {},
   "source": [
    "For strings: use StringLookup to go from strings to integer indices, then use CategoryEncoding to convert to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf3b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, input in inputs.items():\n",
    "    if input.dtype == tf.float32:\n",
    "        continue\n",
    "    lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
    "    one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "    \n",
    "    x = lookup(input)\n",
    "    x = one_hot(x)\n",
    "    preprocessed_inputs.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd103548",
   "metadata": {},
   "source": [
    "now we can concatenate all them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e65285d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "\n",
    "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "\n",
    "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72a718",
   "metadata": {},
   "source": [
    "This is a model that just contains the preprocessing. It should expect a dictionary of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bc3f1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(627, 28), dtype=float32, numpy=\n",
       "array([[-0.61 ,  0.395, -0.479, ...,  0.   ,  1.   ,  0.   ],\n",
       "       [ 0.669,  0.395, -0.479, ...,  0.   ,  1.   ,  0.   ],\n",
       "       [-0.29 , -0.474, -0.479, ...,  0.   ,  0.   ,  1.   ],\n",
       "       ...,\n",
       "       [-0.85 , -0.474, -0.479, ...,  0.   ,  0.   ,  1.   ],\n",
       "       [-0.13 ,  0.395,  2.045, ...,  0.   ,  1.   ,  0.   ],\n",
       "       [ 0.189, -0.474, -0.479, ...,  0.   ,  0.   ,  1.   ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_features_dict = {\n",
    "    name: np.array(value)\n",
    "    for name, value in titanic_features.items()\n",
    "}\n",
    "\n",
    "titanic_preprocessing(titanic_features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88073c",
   "metadata": {},
   "source": [
    "zoom in on a single pass feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e59d5946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 28), dtype=float32, numpy=\n",
       "array([[-0.61 ,  0.395, -0.479, -0.497,  0.   ,  0.   ,  1.   ,  0.   ,\n",
       "         0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "         0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ,\n",
       "         0.   ,  0.   ,  1.   ,  0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
    "titanic_preprocessing(feat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832a25b",
   "metadata": {},
   "source": [
    "build the model on top of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba15ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titanic_model(preprocessing_head, inputs):\n",
    "    body = tf.keras.Sequential([\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # remember that inputs is the raw data (pandas)\n",
    "    # think of `result` as the graph representing what computations\n",
    "    # are to be done. It consists of the preprocessing then\n",
    "    # then linear layers\n",
    "    \n",
    "    # This function is set up to return a model that's ready for fitting\n",
    "    # given some generic preprocessing head\n",
    "    preprocessed_inputs = preprocessing_head(inputs)\n",
    "    result = body(preprocessed_inputs)\n",
    "    model = tf.keras.Model(inputs, result)\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam())\n",
    "    \n",
    "    return model\n",
    "\n",
    "titanic_model = titanic_model(titanic_preprocessing, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbdfa4",
   "metadata": {},
   "source": [
    "Fit the model - remember that we're giving it a dictionary of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b859cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 1s 2ms/step - loss: 0.6332\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5517\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5008\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4698\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4527\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4391\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4316\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4277\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4243\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1e5dfa140>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adad32d",
   "metadata": {},
   "source": [
    "We can save the model and reloadit to get identical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fdbf36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: load_data_tutorial_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: load_data_tutorial_model/assets\n"
     ]
    }
   ],
   "source": [
    "titanic_model.save('load_data_tutorial_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f42cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model('load_data_tutorial_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00bdf30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.896]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[-1.896]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
    "\n",
    "before = titanic_model(features_dict)\n",
    "after = reloaded(features_dict)\n",
    "assert (before-after)<1e-3\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e5078",
   "metadata": {},
   "source": [
    "### Using tf.data\n",
    "\n",
    "This will give us more control over the input data pipeline or use data that doesn't easily fit into memory.\n",
    "\n",
    "Consider the following code to manually slice up the dictionary of features from the last section:L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e15854fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def slices(features):\n",
    "    for i in itertools.count():\n",
    "        # for each features take index `i`\n",
    "        example = {name:values[i] for name, values in features.items()}\n",
    "        yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659411a0",
   "metadata": {},
   "source": [
    "now run and print first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6961c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                : male\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : Third\n",
      "deck               : unknown\n",
      "embark_town        : Southampton\n",
      "alone              : n\n"
     ]
    }
   ],
   "source": [
    "for example in slices(titanic_features_dict):\n",
    "    for name, value in example.items():\n",
    "        print(f\"{name:19s}: {value}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095013e",
   "metadata": {},
   "source": [
    "Most basic dataset: tf.datad.Dataset.from_tensor_slices - generalized version of `slices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7786f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                : b'male'\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : b'Third'\n",
      "deck               : b'unknown'\n",
      "embark_town        : b'Southampton'\n",
      "alone              : b'n'\n"
     ]
    }
   ],
   "source": [
    "features_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    titanic_features_dict\n",
    ")\n",
    "\n",
    "for example in features_ds:\n",
    "    for name, value in example.items():\n",
    "        print(f\"{name:19s}: {value}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d0490",
   "metadata": {},
   "source": [
    "can handle any structure of nested dictionaries or tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a88b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cab61",
   "metadata": {},
   "source": [
    "to train a model: shuffle and batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88d21b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b13783",
   "metadata": {},
   "source": [
    "now we can pass the whole dataset to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80e3f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4220\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4213\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4215\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4192\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1e55f5a80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_model.fit(titanic_batches, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bed7d8",
   "metadata": {},
   "source": [
    "### From a single file\n",
    "\n",
    "functions for loading csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dcfc4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "30874/30874 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c8f36",
   "metadata": {},
   "source": [
    "Now read the csv from the file and make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5802470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/anaconda3/envs/personalenv/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/anaconda3/envs/personalenv/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file_path,\n",
    "    batch_size=5,\n",
    "    label_name='survived',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb421d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female' b'male' b'male']\n",
      "age                 : [32. 36. 31. 50. 28.]\n",
      "n_siblings_spouses  : [0 1 1 1 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [ 7.925 17.4   18.    55.9    7.896]\n",
      "class               : [b'Third' b'Third' b'Third' b'First' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'E' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'y' b'n' b'n' b'n' b'y']\n",
      "\n",
      "label               : [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for batch, label in titanic_csv_ds.take(1):\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "    print()\n",
    "    print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd678f8",
   "metadata": {},
   "source": [
    "can also decompress files on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5e6b55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\n",
      "   8192/Unknown - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "traffic_volume_csv_gz = tf.keras.utils.get_file(\n",
    "    'Metro_Interstate_Traffic_Volume.csv.gz', \n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n",
    "    cache_dir='.', cache_subdir='traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f82fe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holiday             : [b'None' b'None' b'None' b'None' b'None']\n",
      "temp                : [276.29 295.76 252.97 293.77 286.81]\n",
      "rain_1h             : [0. 0. 0. 0. 0.]\n",
      "snow_1h             : [0. 0. 0. 0. 0.]\n",
      "clouds_all          : [90  0  1  0 90]\n",
      "weather_main        : [b'Rain' b'Clear' b'Clear' b'Clear' b'Clouds']\n",
      "weather_description : [b'light rain' b'Sky is Clear' b'sky is clear' b'Sky is Clear'\n",
      " b'overcast clouds']\n",
      "date_time           : [b'2013-01-11 17:00:00' b'2013-09-25 15:00:00' b'2013-02-21 08:00:00'\n",
      " b'2013-08-10 20:00:00' b'2013-05-27 13:00:00']\n",
      "\n",
      "label               : [5668 6000 6250 3333 3436]\n"
     ]
    }
   ],
   "source": [
    "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n",
    "    traffic_volume_csv_gz,\n",
    "    batch_size=256,\n",
    "    label_name='traffic_volume',\n",
    "    num_epochs=1,\n",
    "    compression_type=\"GZIP\")\n",
    "\n",
    "for batch, label in traffic_volume_csv_gz_ds.take(1):\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key:20s}: {value[:5]}\")\n",
    "    print()\n",
    "    print(f\"{'label':20s}: {label[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1de183",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "There's an overhead to parsing csv data. Can be a bottleneck for small models.\n",
    "\n",
    "might want to use `Dataset.cache` or `tf.data.Dataset.snapshot` so csv is only parsed in first epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7f107",
   "metadata": {},
   "source": [
    "### Multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03fddec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\n",
      "   8192/Unknown - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "fonts_zip = tf.keras.utils.get_file(\n",
    "    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n",
    "    cache_dir='.', cache_subdir='fonts',\n",
    "    extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6d2a33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fonts/AGENCY.csv',\n",
       " 'fonts/ARIAL.csv',\n",
       " 'fonts/BAITI.csv',\n",
       " 'fonts/BANKGOTHIC.csv',\n",
       " 'fonts/BASKERVILLE.csv',\n",
       " 'fonts/BAUHAUS.csv',\n",
       " 'fonts/BELL.csv',\n",
       " 'fonts/BERLIN.csv',\n",
       " 'fonts/BERNARD.csv',\n",
       " 'fonts/BITSTREAMVERA.csv']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "font_csvs = sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n",
    "\n",
    "font_csvs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "075eb855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(font_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c301368",
   "metadata": {},
   "source": [
    "You can pass a glob-style `file_pattern` to `make_csv_dataset`. use `num_parallel_reads` to set how many files are read in parallel and interleaved together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc0ddcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = \"fonts/*.csv\",\n",
    "    batch_size=10, num_epochs=1,\n",
    "    num_parallel_reads=20,\n",
    "    shuffle_buffer_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbc96be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font                : [b'PRISTINA' b'BANKGOTHIC' b'NIAGARA' b'SCRIPTB' b'VLADIMIR' b'BAITI'\n",
      " b'VLADIMIR' b'EUROROMAN' b'VINER' b'BAITI']\n",
      "fontVariant         : [b'PRISTINA' b'BANKGOTHIC MD BT' b'NIAGARA SOLID' b'SCRIPT MT BOLD'\n",
      " b'VLADIMIR SCRIPT' b'MONGOLIAN BAITI' b'VLADIMIR SCRIPT' b'EUROROMAN'\n",
      " b'VINER HAND ITC' b'MONGOLIAN BAITI']\n",
      "m_label             : [  960  8720  8482  8364   376 12309  8250 61687  8721  8221]\n",
      "strength            : [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "italic              : [0 0 0 0 0 0 0 0 0 0]\n",
      "orientation         : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "m_top               : [51 27 32 37 23 24 59 36 37 32]\n",
      "m_left              : [22 23 20 19 30 25 27 22 21 23]\n",
      "originalH           : [28 59 27 48 78 51 18  7 58 17]\n",
      "originalW           : [28 34 33 39 62 13 13 21 46 25]\n",
      "h                   : [20 20 20 20 20 20 20 20 20 20]\n",
      "w                   : [20 20 20 20 20 20 20 20 20 20]\n",
      "r0c0                : [  1 255 255   1   1  56   1   1 148   1]\n",
      "r0c1                : [  1 255 255   1   1 102   1   1 255 103]\n",
      "r0c2                : [ 63 255 217   1   1 156   1   1 255 255]\n",
      "r0c3                : [ 73 255 213   1   1 248   1  49 255 255]\n",
      "...\n",
      "[total: 412 features]\n"
     ]
    }
   ],
   "source": [
    "for features in fonts_ds.take(1):\n",
    "    for i, (name, value) in enumerate(features.items()):\n",
    "        if i>15:\n",
    "            break\n",
    "        print(f\"{name:20s}: {value}\")\n",
    "print('...')\n",
    "print(f\"[total: {len(features)} features]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093d6be",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.tensorflow.org/tutorials/load_data/csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
