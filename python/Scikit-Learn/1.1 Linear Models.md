**Important articles & models / methods that I need to look into**<br> 
- https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py
- https://en.wikipedia.org/wiki/Akaike_information_criterion
- https://en.wikipedia.org/wiki/Bayesian_information_criterion
- Multi-Task Lasso



# 1.1 - Linear Models ([source](https://scikit-learn.org/stable/modules/linear_model.html))

Linear models assume the target value is some linear combination of the features. The general model is in the form: 

$$
\hat y(\omega, x) = \omega_0 + \omega_1 x_1 + \cdot \cdot \cdot + \omega_p x_p
$$

where: 
- $\hat y$ = target value (dependent variable)
- $(x_1, . . ., x_p)$ = input values (independent variable(s))
- $\omega = (\omega_1, . . ., \omega_p)$ = weights (`coef_`)
- $\omega_0$ = bias (`intercept_`)

## 1.1.1 Ordinary Least Squares

`LinearRegression()` - minimize the sum of squares

$$
\operatorname{min} \left\lVert X\omega - y \right\rVert_2^2
$$

It assumes independence of features. When som efeatures are correlated, the estimate can become sensitive to random errors b/c of a singularity in $X$

non-negative least squares: setting `positive=True` in constructor method constrains all coefficients to $>0$

complexity: $\mathcal{O}(n_{\text{samples}}n_{\text{features}}^2)$, assuming $n_{\text{samples}} \ge n_{\text{features}}$

## 1.1.2 Ridge Regression & Classification

`Ridge()` Imposes $l_2$ regularization on the coefficients: 

$$
\operatorname{min} \left\lVert X\omega - y \right\rVert_2^2 + \alpha \left\lVert \omega  \right\rVert_2^2
$$


```python
# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>
# License: BSD 3 clause

print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# X is the 10x10 Hilbert matrix
X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
y = np.ones(10)

# #############################################################################
# Compute paths

n_alphas = 200
alphas = np.logspace(-10, -2, n_alphas)

coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X, y)
    coefs.append(ridge.coef_)

# #############################################################################
# Display results

ax = plt.gca()

ax.plot(alphas, coefs)
ax.set_xscale('log')
ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
plt.xlabel('alpha')
plt.ylabel('weights')
plt.title('Ridge coefficients as a function of the regularization')
plt.axis('tight')
plt.show()
```

    Automatically created module for IPython interactive environment



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_4_1.png)
    


## Classification

`RidgeClassifier` - converts binary targets to {-1, 1} then treats the problem as a regression task. This can be thought of as a least squares support vector machines with a linear kernel. Can be faster than logistic regression with many classes. Only computes projection matrix once.

**RidgeCV** uses ridge regression with automatic cross validation of alpha. Works the same way as GridSearchCV with Leave-One-Out CV. specifying cv attribute triggers GridSearchCV with k-fold CV. 

# 1.1.3 Lasso

Estimate spare coefficients. Sometimes useful becuase it tends to prefer solutions with fewer non-zero coefficients. Since it uses the more aggressive $l_1$ regularization, it may be used as a method of feature selection (i.e. all the non-important coeffients become zero.
<br> 
`lasso_path` - computes coefficients along full path of possible values 

$$
\operatorname{min} \frac{1}{2n_{\text{samples}}}\left\lVert X\omega - y \right\rVert_2^2 + \alpha \left\lVert \omega  \right\rVert_1
$$

Cross-validation: `LassoCV` and `LassoLarsCV`. `LassoLarsCV` is based on Least Angle Regression. LassoCV may be preferable with high-d datasets w/ collinear features. LassoLarsCV can explore more relevant values of alpha. <br> 
<br> 
`LassoLarsIC` - uses the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [Bayes information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). Need to read more into this<br> 
<br> 
Comparison between alpha and regularization parameter of SVM (c): $\alpha = 1/c$


```python
print(__doc__)

# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort
# License: BSD 3 clause

import time

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC
from sklearn import datasets

# This is to avoid division by zero while doing np.log10
EPSILON = 1e-4

X, y = datasets.load_diabetes(return_X_y=True)

rng = np.random.RandomState(42)
X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features

# normalize data as done by Lars to allow for comparison
X /= np.sqrt(np.sum(X ** 2, axis=0))

# #############################################################################
# LassoLarsIC: least angle regression with BIC/AIC criterion

model_bic = LassoLarsIC(criterion='bic')
t1 = time.time()
model_bic.fit(X, y)
t_bic = time.time() - t1
alpha_bic_ = model_bic.alpha_

model_aic = LassoLarsIC(criterion='aic')
model_aic.fit(X, y)
alpha_aic_ = model_aic.alpha_


def plot_ic_criterion(model, name, color):
    criterion_ = model.criterion_
    plt.semilogx(model.alphas_ + EPSILON, criterion_, '--', color=color,
                 linewidth=3, label='%s criterion' % name)
    plt.axvline(model.alpha_ + EPSILON, color=color, linewidth=3,
                label='alpha: %s estimate' % name)
    plt.xlabel(r'$\alpha$')
    plt.ylabel('criterion')


plt.figure()
plot_ic_criterion(model_aic, 'AIC', 'b')
plot_ic_criterion(model_bic, 'BIC', 'r')
plt.legend()
plt.title('Information-criterion for model selection (training time %.3fs)'
          % t_bic)

# #############################################################################
# LassoCV: coordinate descent

# Compute paths
print("Computing regularization path using the coordinate descent lasso...")
t1 = time.time()
model = LassoCV(cv=20).fit(X, y)
t_lasso_cv = time.time() - t1

# Display results
plt.figure()
ymin, ymax = 2300, 3800
plt.semilogx(model.alphas_ + EPSILON, model.mse_path_, ':')
plt.plot(model.alphas_ + EPSILON, model.mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(model.alpha_ + EPSILON, linestyle='--', color='k',
            label='alpha: CV estimate')

plt.legend()

plt.xlabel(r'$\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent '
          '(train time: %.2fs)' % t_lasso_cv)
plt.axis('tight')
plt.ylim(ymin, ymax)

# #############################################################################
# LassoLarsCV: least angle regression

# Compute paths
print("Computing regularization path using the Lars lasso...")
t1 = time.time()
model = LassoLarsCV(cv=20).fit(X, y)
t_lasso_lars_cv = time.time() - t1

# Display results
plt.figure()
plt.semilogx(model.cv_alphas_ + EPSILON, model.mse_path_, ':')
plt.semilogx(model.cv_alphas_ + EPSILON, model.mse_path_.mean(axis=-1), 'k',
             label='Average across the folds', linewidth=2)
plt.axvline(model.alpha_, linestyle='--', color='k',
            label='alpha CV')
plt.legend()

plt.xlabel(r'$\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
          % t_lasso_lars_cv)
plt.axis('tight')
plt.ylim(ymin, ymax)

plt.show()
```

    Automatically created module for IPython interactive environment
    Computing regularization path using the coordinate descent lasso...
    Computing regularization path using the Lars lasso...



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_7_1.png)
    



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_7_2.png)
    



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_7_3.png)
    


# 1.1.4 Multi-task Lasso

I barely have any idea what's going on here- just going to put down a bunch of equations and I'll come back to this later after I've read more. I'm pretty sure this refers to a model that's trying to predict more than one output. 

Linear model with mixed $l_1 l_2$ norm. 

Objective function for minimization: 
$$
\operatorname{min} \frac{1}{2n_{\text{samples}}}\left\lVert XW - Y \right\rVert_{\operatorname{Fro}}^2 + \alpha \left\lVert W  \right\rVert_{21}
$$

$\operatorname{Fro}$ = frobenius norm

$$
\left\lVert A  \right\rVert_{\operatorname{Fro}} = \sqrt{\sum_{ij}{\alpha_{ij}^2}}
$$

$l_1 l_2$ reads

$$
\left\lVert A  \right\rVert_{21} = \sum_i{\sqrt{\sum_{j}{\alpha_{i}^2}}}
$$


```python
print(__doc__)

# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause

import matplotlib.pyplot as plt
import numpy as np

from sklearn.linear_model import MultiTaskLasso, Lasso

rng = np.random.RandomState(42)

# Generate some 2D coefficients with sine waves with random frequency and phase
n_samples, n_features, n_tasks = 100, 30, 40
n_relevant_features = 5
coef = np.zeros((n_tasks, n_features))
times = np.linspace(0, 2 * np.pi, n_tasks)
for k in range(n_relevant_features):
    coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))

X = rng.randn(n_samples, n_features)
Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)

coef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])
coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_

# #############################################################################
# Plot support and time series
fig = plt.figure(figsize=(8, 5))
plt.subplot(1, 2, 1)
plt.spy(coef_lasso_)
plt.xlabel('Feature')
plt.ylabel('Time (or Task)')
plt.text(10, 5, 'Lasso')
plt.subplot(1, 2, 2)
plt.spy(coef_multi_task_lasso_)
plt.xlabel('Feature')
plt.ylabel('Time (or Task)')
plt.text(10, 5, 'MultiTaskLasso')
fig.suptitle('Coefficient non-zero location')

feature_to_plot = 0
plt.figure()
lw = 2
plt.plot(coef[:, feature_to_plot], color='seagreen', linewidth=lw,
         label='Ground truth')
plt.plot(coef_lasso_[:, feature_to_plot], color='cornflowerblue', linewidth=lw,
         label='Lasso')
plt.plot(coef_multi_task_lasso_[:, feature_to_plot], color='gold', linewidth=lw,
         label='MultiTaskLasso')
plt.legend(loc='upper center')
plt.axis('tight')
plt.ylim([-1.1, 1.1])
plt.show()
```

    Automatically created module for IPython interactive environment



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_9_1.png)
    



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_9_2.png)
    


# Elastic-Net

Combination of $l_1$ and $l_2$ norms. can learn a sparse model with few nonzero weights but with the regularization properties of ridge. The convex combination of $l_1$ and $l_2$ is controlled with the l1_ratio parameter


Useful when there are multiple correlated features. Lasso might pick one at random, elastic-net can pick both. 

Inherits some of Ridge's stability under rotation (*What does this mean?*)

Objective function: 

$$
\operatorname{min} \frac{1}{2n_{\text{samples}}}\left\lVert X\omega -y \right\rVert_{2}^2 + \alpha \rho \left\lVert \omega  \right\rVert_{1} + \frac{\alpha (1-\rho)}{2} \left\lVert \omega \right\rVert_2^2
$$

- $\rho$ = l1_ratio
- `ElasticNetCV` can set $\alpha$ and $\rho$ by cross validation



```python
print(__doc__)

# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause

from itertools import cycle
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import lasso_path, enet_path
from sklearn import datasets


X, y = datasets.load_diabetes(return_X_y=True)


X /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)

# Compute paths

eps = 5e-3  # the smaller it is the longer is the path

print("Computing regularization path using the lasso...")
alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps, fit_intercept=False)

print("Computing regularization path using the positive lasso...")
alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(
    X, y, eps=eps, positive=True, fit_intercept=False)
print("Computing regularization path using the elastic net...")
alphas_enet, coefs_enet, _ = enet_path(
    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)

print("Computing regularization path using the positive elastic net...")
alphas_positive_enet, coefs_positive_enet, _ = enet_path(
    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)

# Display results

plt.figure(1)
colors = cycle(['b', 'r', 'g', 'c', 'k'])
neg_log_alphas_lasso = -np.log10(alphas_lasso)
neg_log_alphas_enet = -np.log10(alphas_enet)
for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):
    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and Elastic-Net Paths')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')
plt.axis('tight')


plt.figure(2)
neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)
for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):
    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and positive Lasso')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')
plt.axis('tight')


plt.figure(3)
neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)
for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):
    l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)
    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Elastic-Net and positive Elastic-Net')
plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),
           loc='lower left')
plt.axis('tight')
plt.show()
```

    Automatically created module for IPython interactive environment
    Computing regularization path using the lasso...
    Computing regularization path using the positive lasso...
    Computing regularization path using the elastic net...
    Computing regularization path using the positive elastic net...



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_11_1.png)
    



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_11_2.png)
    



    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_11_3.png)
    


# 1.1.6 Multi-task Elastic-Net

Does what it says on the tin. 

$$
\operatorname{min} \frac{1}{2n_{\text{samples}}}\left\lVert XW - Y \right\rVert_{\operatorname{Fro}}^2 + \alpha \rho \left\lVert W  \right\rVert_{21} + \frac{\alpha (1-\rho)}{2} \left\lVert W \right\rVert_{\operatorname{Fro}}^2
$$

# 1.1.7 Least Angle Regression

- Good for high dimensional data
- Similar to forward stepwise regression
- w/ each step, it finds the feature most correlated with target. Proceeds in a direction equiangular between features. 

**Advantages of LARS**

- efficient when n features >> n samples
- as fast as forward selection w/ same order of complexity as OLS
- full peicewise linear solution path
- easily modified for other estimators

**Disadvantages**

- succeptable to noise

[paper](https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)

# 1.1.8 Lars Lasso

Lasso model implemented using LARS. Gives the full path of the coefficients along regularization parameter. Instead of a vector, this gives a curve showing the solution for each value of the $l_1$ norm fof the parameter vector. 


```python
print(__doc__)

# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>
#         Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model
from sklearn import datasets

X, y = datasets.load_diabetes(return_X_y=True)

print("Computing regularization path using the LARS ...")
_, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)

xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]

plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle='dashed')
plt.xlabel('|coef| / max|coef|')
plt.ylabel('Coefficients')
plt.title('LASSO Path')
plt.axis('tight')
plt.show()
```

    Automatically created module for IPython interactive environment
    Computing regularization path using the LARS ...
    .


    
![png](1.1%20Linear%20Models_files/1.1%20Linear%20Models_15_1.png)
    


# 1.1.9 Orthogonal Matching Pursuit (OMP)

Approximates the fit of a linear model with constaints imposed on the number of non-zero coefficients ($l_0$ pseudo-norm)

Approximate optimum solution vector with a fixed number on non-zero arguments: 

$$
\operatorname{arg min} \left\lVert y - X \omega \right\rVert_2^2 \text{  subject to} \left\lVert \omega \right\rVert_0 \le n_{\text{nonzero\_coefs}}
$$

Can also target a specific error: 
$$
\operatorname{arg min} \left\lVert \omega \right\rVert_0  \text{  subject to}  \left\lVert y - X \omega \right\rVert_2^2 \le \text{tol}
$$

greedy- includes at each step the atom most highly correlated wtih the current residual. Similar to Matching Pursuit (MP), but the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements. 

**Q:** is this $l_0$ norm like a delta function? 


https://math.stackexchange.com/questions/393432/why-is-ell-0-a-pseudo-norm

https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf

# 1.1.10 Bayesian Regression

Includes regularization parameters in the estimation. 

Introduce *uninformative priors* over hyperparameters for the model. 

Here, the output $y$ is assumed to be a Gaussian distribution around $X\omega$

$$
p(y | X, \omega, \alpha) = N(y | X\omega, \alpha)
$$

advantages: adapts to data at hand. Can include regularization parameters in estimation.<br> 
disadvantages: inference can be time consuming. 

helpful lecture: https://www.youtube.com/watch?v=LzZ5b3wdZQk

In plain English, we're calculating the probability that the model has this given set of parameters given a list of features ((x, y) pairs in the training data). It's using the 'big picture' to figure out the parameters given some data.

### Bayesian Ridge Regression

The prior for the coefficient $\omega$ is given by a spherical gaussian: 

$$
p(\omega | \lambda) = N(\omega | 0, \lambda^{-1}I_p)
$$


```python

```
