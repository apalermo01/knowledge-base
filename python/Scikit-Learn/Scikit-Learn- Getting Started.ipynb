{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funny-farming",
   "metadata": {},
   "source": [
    "# Getting started ([source](https://scikit-learn.org/stable/getting_started.html#))\n",
    "\n",
    "Review of the basic functionality of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-afghanistan",
   "metadata": {},
   "source": [
    "# Fitting and predicting: Estimator basics\n",
    "\n",
    "**Estimator** - built in machine learning algorithms / models<br> \n",
    "Each estimator has a fit() method to fit data. \n",
    "\n",
    "Example with random forest classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suitable-adult",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Declare a small dataset with 2 samples and 3 features\n",
    "X = [[1, 2, 3], \n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-precipitation",
   "metadata": {},
   "source": [
    "fit method usually takes 2 inputs: matrix of features (n_samples x n_features), and target values y (regression: real numbers; classification: integers).<br> \n",
    "\n",
    "after fitting, predict new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indonesian-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-adams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[4, 5, 6], [14, 15, 16]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-nashville",
   "metadata": {},
   "source": [
    "# Transformers and pre-processors\n",
    "\n",
    "preprocessors and transformers have most of the same methods as estimators. Transformers have no predict method, instead they have transform() which returns the new sample matrix. ColumnTransformer can apply different transformations to different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 15], \n",
    "     [1, -10]]\n",
    "\n",
    "# Scale data according to computed scaling values\n",
    "StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-border",
   "metadata": {},
   "source": [
    "# Pipelines: chining pre-processors and estimators\n",
    "\n",
    "Combine transformers and estimators into a pipeline (same API as the estimator). Prevents data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weighted-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a pipeline object\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# load iris and split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# fit entire pipeline\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equal-document",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now use it like any estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-clearing",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Perform a 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorporated-module",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = make_regression(n_samples=1000, random_state=0)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "result = cross_validate(lr, X, y)\n",
    "result['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-briefs",
   "metadata": {},
   "source": [
    "# Automatic parameter searches\n",
    "\n",
    "All estimators have tunable parameters (*hyperparameters*). Automatically find the best parameter combinations using cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "natural-crazy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001810FAEDB88>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001810FB38408>},\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# define the parameter space\n",
    "param_distributions = {'n_estimators': randint(1, 5), \n",
    "                       'max_depth': randint(5, 10)}\n",
    "\n",
    "# create a searchCV object and fit \n",
    "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), \n",
    "                           n_iter=5, \n",
    "                           param_distributions=param_distributions, \n",
    "                           random_state=0)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distinct-topic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "devoted-panic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735363411343253"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search now acts like a normal estimator with the best parameters\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-chemical",
   "metadata": {},
   "source": [
    "**NOTE**: it's always best to search over a pipeline, not a single estimator. If you preprocess the entire dataset at once, some info about the training set may leak into the test set, violating the assumption that the sets are independent. ([related kaggle post](https://www.kaggle.com/alexisbcook/data-leakage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITCS_5156",
   "language": "python",
   "name": "itcs_5156"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
