{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a3613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7e943",
   "metadata": {},
   "source": [
    "---\n",
    "# PyTorch Lightning\n",
    "\n",
    "1. Model\n",
    "2. Optimizer\n",
    "3. Data\n",
    "4. training loop \"the magic\"\n",
    "5. validation loop \"the validation magic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e405390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "class ResNet(pl.LightningModule):\n",
    "  \"\"\"\n",
    "  This is exactly the same as an nn module\n",
    "  \n",
    "  just with some extra optional ingredients\n",
    "  \n",
    "  NOTE: no need for .cuda()- lightning does that for us\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.l1 = nn.Linear(28*28, 64)\n",
    "    self.l2 = nn.Linear(64, 64)\n",
    "    self.l3 = nn.Linear(64, 10)\n",
    "    self.do = nn.Dropout(0.1)\n",
    "    self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = nn.functional.relu(self.l1(x))\n",
    "    h2 = nn.functional.relu(self.l2(h1))\n",
    "    do = self.do(h2+h1)\n",
    "    logits = self.l3(do)\n",
    "    return logits\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"\n",
    "    pl function- can configure as many optimizers as we want\n",
    "    pl gives us a train loop for each optimizer\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(self.parameters(), lr=1e-2)\n",
    "    return optimizer\n",
    "  \n",
    "  ### training loop\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    \"\"\"\n",
    "    pl function - implements training loop. \n",
    "    this is the magic\n",
    "    \"\"\"\n",
    "    x, y = batch\n",
    "    \n",
    "    # x: b x 1 x 28 x 28\n",
    "    b = x.size(0)\n",
    "    x = x.view(b, -1)\n",
    "    \n",
    "    # 1 forward\n",
    "    logits = self(x) #model(x) # l: logits\n",
    "    \n",
    "    # 2 compute objective function\n",
    "    J = self.loss(logits, y)\n",
    "    \n",
    "    # lightning detaches automatically, need to return with graph attached. \n",
    "    # return J\n",
    "  \n",
    "    # calculate accuracy\n",
    "    # metrics can be automatically calculated across all gpus for multi-gpu training\n",
    "    acc = accuracy(logits, y)\n",
    "    pbar = {'train_acc': acc}\n",
    "\n",
    "    # equivalently\n",
    "    # 3 reserved words: 'log', 'loss', 'progress_bar'\n",
    "    return {'loss': J, 'progress_bar': pbar}\n",
    "\n",
    "#   def backward(self, trainer, loss, optimizer, optimizer_idx):\n",
    "#     \"\"\"\n",
    "#     This method is implemented for us, but if we want we can override it for custom functionality\n",
    "#     \"\"\"\n",
    "#     loss.backward()\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    \"\"\"\n",
    "    use this if we need to figure out the number of classes\n",
    "    \"\"\"\n",
    "    train_data = datasets.MNIST('data', train=True, download=False, transform=transforms.ToTensor())\n",
    "    self.train, self.val = random_split(train_data, [55000, 5000])\n",
    "    train_loader = DataLoader(self.train, batch_size=32)\n",
    "    #val_loader = DataLoader(val, batch_size=32)\n",
    "    return train_loader\n",
    "  \n",
    "  def val_dataloader(self):\n",
    "\n",
    "    val_loader = DataLoader(self.val, batch_size=32)\n",
    "    return val_loader\n",
    "\n",
    "  ### 2 methods for validation loop: validation_step, \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    \"\"\"\n",
    "    We generally don't want metrics for every batch. plot for whole validation set.\n",
    "    For every single batch in the validation loop, get the accuracy & loss. Lightning will cache it all for us\n",
    "    \"\"\"\n",
    "    results = self.training_step(batch, batch_idx)\n",
    "    results['progress_bar']['val_acc'] = results['progress_bar']['train_acc']\n",
    "    del results['progress_bar']['train_acc']\n",
    "    return results\n",
    "\n",
    "  def validation_epoch_end(self, val_step_outputs):\n",
    "    # [results, results, results, results, ...]\n",
    "    # calcualte avg val loss for all val outputs\n",
    "    avg_val_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n",
    "    avg_acc = torch.tensor([x['progress_bar']['val_acc'] for x in val_step_outputs]).mean()\n",
    "    # note: early stopping is implemented automatically\n",
    "    pbar = {'avg_val_acc': avg_val_acc}\n",
    "    return {'val_loss': avg_val_loss, 'progress_bar': pbar} # val loss is all we care about for early stopping / checkpoint\n",
    "\n",
    "model = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca23726",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/alex/anaconda3/envs/pl_lightning/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | l1   | Linear           | 50.2 K\n",
      "1 | l2   | Linear           | 4.2 K \n",
      "2 | l3   | Linear           | 650   \n",
      "3 | do   | Dropout          | 0     \n",
      "4 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047e77725fb14cc6becf1fbd35a6590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(progress_bar_refresh_rate=20,\n",
    "                     max_epochs=5,\n",
    "                     gpus=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e073622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_0  version_1  version_2  version_3  version_4  version_5\r\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900f4b1",
   "metadata": {},
   "source": [
    "lightning saved the best checkpoint for us plus logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c29b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.resnet = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f548d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/pl_lightning/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630841592/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# train, val split\n",
    "train_data = datasets.MNIST('data', train=True, download=False, transform=transforms.ToTensor())\n",
    "train, val = random_split(train_data, [55000, 5000])\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5ce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(28*28, 64),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(64, 64), \n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(0.1),\n",
    "  nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a600a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "params = model.parameters()\n",
    "optimiser = optim.SGD(params, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d78da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0d5ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 0.84, training accuracy: 0.78\n",
      "Epoch 1, validation loss: 0.40, validation accuracy: 0.89\n",
      "Epoch 2, training loss: 0.38, training accuracy: 0.89\n",
      "Epoch 2, validation loss: 0.33, validation accuracy: 0.91\n",
      "Epoch 3, training loss: 0.31, training accuracy: 0.91\n",
      "Epoch 3, validation loss: 0.28, validation accuracy: 0.92\n",
      "Epoch 4, training loss: 0.27, training accuracy: 0.92\n",
      "Epoch 4, validation loss: 0.25, validation accuracy: 0.93\n",
      "Epoch 5, training loss: 0.24, training accuracy: 0.93\n",
      "Epoch 5, validation loss: 0.23, validation accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "# training and validation loops\n",
    "nb_epochs = 5\n",
    "for epoch in range(nb_epochs):\n",
    "  losses = list()\n",
    "  accuracies = list()\n",
    "  model.train()\n",
    "  for batch in train_loader:\n",
    "    x, y = batch\n",
    "    \n",
    "    # x: b x 1 x 28 x 28\n",
    "    b = x.size(0)\n",
    "    x = x.view(b, -1)\n",
    "    \n",
    "    # 1 forward\n",
    "    l = model(x) # l: logits\n",
    "    \n",
    "    # 2 compute objective function\n",
    "    J = loss(l, y)\n",
    "    \n",
    "    # 3 clearning the gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # accumulate the partial derivatives of J wrt params\n",
    "    J.backward()\n",
    "    \n",
    "    # 5 step in the opposite direction of the gradient\n",
    "    optimiser.step()\n",
    "    \n",
    "    losses.append(J.item())\n",
    "    accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())\n",
    "    \n",
    "  print(f'Epoch {epoch+1}', end=', ')\n",
    "  print(f'training loss: {torch.tensor(losses).mean():.2f}', end=', ')\n",
    "  print(f'training accuracy: {torch.tensor(accuracies).mean():.2f}')\n",
    "  \n",
    "  losses = list()\n",
    "  accuracies = list()\n",
    "  model.eval()\n",
    "  for batch in val_loader:\n",
    "    x, y = batch\n",
    "    \n",
    "    # x: b x 1 x 28 x 28\n",
    "    b = x.size(0)\n",
    "    x = x.view(b, -1)\n",
    "    \n",
    "    # 1 forward\n",
    "    with torch.no_grad():\n",
    "      l = model(x) # l: logits\n",
    "    \n",
    "    # 2 compute objective function\n",
    "    J = loss(l, y)\n",
    "    \n",
    "    losses.append(J.item())\n",
    "    accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "  print(f'Epoch {epoch+1}', end=', ')\n",
    "  print(f'validation loss: {torch.tensor(losses).mean():.2f}', end=', ')\n",
    "  print(f'validation accuracy: {torch.tensor(accuracies).mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df434ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
